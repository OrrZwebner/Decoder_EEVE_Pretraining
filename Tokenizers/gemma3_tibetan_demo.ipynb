{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemma 3 Tokenizer Expansion Demo - Tibetan Language\n",
        "\n",
        "This notebook demonstrates the `expand_tokenizer` function using Gemma 3 model with Tibetan corpus. We'll show tokenization before and after training on 100 new tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\◊ê◊®◊ñ\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from typing import List\n",
        "\n",
        "# Add Tokenizers directory to path for imports\n",
        "# In Jupyter notebooks, os.getcwd() returns the directory where notebook is located\n",
        "# If notebook is in Tokenizers/, this will work. Otherwise adjust the path.\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from model_processors import get_processor\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Tibetan Corpus and Sample Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tibetan corpus size: 500 samples\n",
            "Sample text: ‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω¢‡æí‡æ±‡Ω£‡ºã‡ΩÅ‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ωë‡Ω∫‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\n"
          ]
        }
      ],
      "source": [
        "# Tibetan corpus for training\n",
        "TIBETAN_CORPUS = [\n",
        "    \"‡Ωñ‡Ωº‡Ωë‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ωñ‡Ωº‡Ωë‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ωò‡Ω≤‡ºã‡Ω¢‡Ω≤‡ΩÇ‡Ω¶‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\",\n",
        "    \"‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\",\n",
        "    \"‡Ωñ‡Ωº‡Ωë‡ºã‡Ω£‡æó‡Ωº‡ΩÑ‡Ω¶‡ºã‡Ωì‡Ω≤‡ºã‡Ω£‡æ∑‡Ωº‡ºã‡Ω¢‡æí‡æ±‡ºã‡ΩÇ‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã‡Ωñ‡Ω£‡ºã‡Ω°‡Ω¥‡Ω£‡ºã‡Ωë‡ΩÑ‡ºã‡Ω†‡Ωñ‡æ≤‡Ω∫‡Ω£‡ºã‡Ωñ‡ºã‡Ω°‡Ωº‡Ωë‡ºç\",\n",
        "    \"‡Ωë‡ΩÄ‡Ω†‡ºã‡Ω£‡Ω¶‡ºã‡Ωò‡ΩÑ‡ºã‡Ωî‡Ωº‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω¥‡ΩÑ‡ºã‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\",\n",
        "    \"‡Ωñ‡Ωº‡Ωë‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¢‡Ω≤‡ΩÇ‡ºã‡ΩÇ‡Ωì‡Ω¶‡ºã‡Ωë‡ΩÑ‡ºã‡Ω£‡Ωº‡ºã‡Ω¢‡æí‡æ±‡Ω¥‡Ω¶‡ºã‡Ωì‡Ω≤‡ºã‡Ωß‡ºã‡ΩÖ‡ΩÑ‡ºã‡Ω¢‡Ω≤‡ΩÑ‡ºã‡Ωî‡Ωº‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\",\n",
        "] * 100  # Repeat for more data\n",
        "\n",
        "# Sample Tibetan text for demonstration (different from corpus)\n",
        "SAMPLE_TIBETAN_TEXT = \"‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω¢‡æí‡æ±‡Ω£‡ºã‡ΩÅ‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ωë‡Ω∫‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\"\n",
        "\n",
        "print(f\"Tibetan corpus size: {len(TIBETAN_CORPUS)} samples\")\n",
        "print(f\"Sample text: {SAMPLE_TIBETAN_TEXT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Original Gemma 3 Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 00:38:28,962 - model_processors - INFO - Loaded google/gemma-3-1b-it tokenizer\n",
            "2025-11-12 00:38:28,962 - model_processors - INFO - Initialized Gemma processor\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded Gemma 3 tokenizer\n",
            "Original vocabulary size: 262,145\n"
          ]
        }
      ],
      "source": [
        "# Model configuration for Gemma 3\n",
        "model_config = {\n",
        "    \"model_name\": \"google/gemma-3-1b-it\",\n",
        "    \"algorithm\": \"SENTENCEPIECE_BPE\"\n",
        "}\n",
        "\n",
        "# Get processor and original tokenizer\n",
        "processor = get_processor(\"gemma\", model_config)\n",
        "original_tokenizer = processor.original_tokenizer\n",
        "\n",
        "# Display original vocabulary size\n",
        "original_vocab_size = len(original_tokenizer.get_vocab())\n",
        "print(f\"‚úÖ Loaded Gemma 3 tokenizer\")\n",
        "print(f\"Original vocabulary size: {original_vocab_size:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenization BEFORE Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "BEFORE TRAINING\n",
            "================================================================================\n",
            "\n",
            "Sample text: ‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω¢‡æí‡æ±‡Ω£‡ºã‡ΩÅ‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ωë‡Ω∫‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\n",
            "\n",
            "Token IDs: [2, 240332, 144766, 242070, 212637, 240392, 55765, 203431, 100206, 242268, 155394, 242079, 151310, 202660, 73436, 242070, 212637, 240451, 240640, 239938, 241594, 236743, 52855, 242545, 235862, 239938, 120295, 239502, 243797, 239985, 155394, 239935, 243835, 240640, 92089, 157435, 201958, 239938, 239935, 239985, 239502, 241594]\n",
            "\n",
            "Number of tokens: 42\n",
            "\n",
            "Token strings:\n",
            "  [0] ID:      2 | Token: '<bos>'\n",
            "  [1] ID: 240332 | Token: '‡Ωñ'\n",
            "  [2] ID: 144766 | Token: '‡Ωº‡Ωë‡ºã'\n",
            "  [3] ID: 242070 | Token: '‡Ω°'\n",
            "  [4] ID: 212637 | Token: '‡Ω≤‡ΩÇ‡ºã'\n",
            "  [5] ID: 240392 | Token: '‡Ωì'\n",
            "  [6] ID:  55765 | Token: '‡Ω≤‡ºã'\n",
            "  [7] ID: 203431 | Token: '‡Ω¢‡æí‡æ±'\n",
            "  [8] ID: 100206 | Token: '‡Ω£‡ºã'\n",
            "  [9] ID: 242268 | Token: '‡ΩÅ'\n",
            "  [10] ID: 155394 | Token: '‡Ωñ‡ºã'\n",
            "  [11] ID: 242079 | Token: '‡ΩÄ'\n",
            "  [12] ID: 151310 | Token: '‡æ±‡Ω≤‡ºã'\n",
            "  [13] ID: 202660 | Token: '‡Ω¶‡æê'\n",
            "  [14] ID:  73436 | Token: '‡Ωë‡ºã'\n",
            "  [15] ID: 242070 | Token: '‡Ω°'\n",
            "  [16] ID: 212637 | Token: '‡Ω≤‡ΩÇ‡ºã'\n",
            "  [17] ID: 240451 | Token: '‡Ω¢'\n",
            "  [18] ID: 240640 | Token: '‡Ω∫'\n",
            "  [19] ID: 239938 | Token: '‡Ωë'\n",
            "  [20] ID: 241594 | Token: '‡ºç'\n",
            "  [21] ID: 236743 | Token: '‚ñÅ'\n",
            "  [22] ID:  52855 | Token: '‡ΩÑ‡ºã'\n",
            "  [23] ID: 242545 | Token: '‡Ωö'\n",
            "  [24] ID: 235862 | Token: '‡Ωº‡Ω¶‡ºã'\n",
            "  [25] ID: 239938 | Token: '‡Ωë'\n",
            "  [26] ID: 120295 | Token: '‡Ω∫‡ºã'\n",
            "  [27] ID: 239502 | Token: '‡Ω¶'\n",
            "  [28] ID: 243797 | Token: '‡æ≥'\n",
            "  [29] ID: 239985 | Token: '‡Ωº'\n",
            "  [30] ID: 155394 | Token: '‡Ωñ‡ºã'\n",
            "  [31] ID: 239935 | Token: '‡ΩÇ'\n",
            "  [32] ID: 243835 | Token: '‡Ωâ'\n",
            "  [33] ID: 240640 | Token: '‡Ω∫'\n",
            "  [34] ID:  92089 | Token: '‡Ω¢‡ºã'\n",
            "  [35] ID: 157435 | Token: '‡Ωñ‡æ±'\n",
            "  [36] ID: 201958 | Token: '‡Ω∫‡Ωë‡ºã'\n",
            "  [37] ID: 239938 | Token: '‡Ωë'\n",
            "  [38] ID: 239935 | Token: '‡ΩÇ'\n",
            "  [39] ID: 239985 | Token: '‡Ωº'\n",
            "  [40] ID: 239502 | Token: '‡Ω¶'\n",
            "  [41] ID: 241594 | Token: '‡ºç'\n",
            "\n",
            "Vocabulary size: 262,145\n"
          ]
        }
      ],
      "source": [
        "# Encode sample text with original tokenizer\n",
        "before_token_ids = original_tokenizer.encode(SAMPLE_TIBETAN_TEXT)\n",
        "before_tokens = original_tokenizer.convert_ids_to_tokens(before_token_ids)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BEFORE TRAINING\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nSample text: {SAMPLE_TIBETAN_TEXT}\")\n",
        "print(f\"\\nToken IDs: {before_token_ids}\")\n",
        "print(f\"\\nNumber of tokens: {len(before_token_ids)}\")\n",
        "print(f\"\\nToken strings:\")\n",
        "for i, (token_id, token) in enumerate(zip(before_token_ids, before_tokens)):\n",
        "    print(f\"  [{i}] ID: {token_id:6d} | Token: '{token}'\")\n",
        "print(f\"\\nVocabulary size: {original_vocab_size:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train and Expand Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TRAINING TOKENIZER\n",
            "================================================================================\n",
            "\n",
            "Training on 500 Tibetan corpus samples...\n",
            "Target: Add 100 new tokens\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 00:38:31,712 - model_processors - INFO - \n",
            "=== EXPANDING GOOGLE/GEMMA-3-1B-IT WITH SENTENCEPIECE_BPE ===\n",
            "2025-11-12 00:38:31,712 - model_processors - INFO - Original vocabulary size: 262,145\n",
            "2025-11-12 00:38:31,712 - model_processors - INFO - Training corpus size: 500\n",
            "2025-11-12 00:38:31,712 - model_processors - INFO - Using train_new_from_iterator approach for SENTENCEPIECE_BPE with 500 corpus samples\n",
            "2025-11-12 00:38:31,712 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "2025-11-12 00:38:34,496 - model_processors - INFO - Base tokenizer model type: BPE\n",
            "2025-11-12 00:38:34,496 - model_processors - INFO - New tokenizer model type: BPE\n",
            "2025-11-12 00:38:34,512 - model_processors - INFO - Trying 1.5x buffer: target vocab size = 150 (150 new tokens to select 100 best from)\n",
            "2025-11-12 00:38:34,512 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "Adding merges & tokens:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 105/111 [00:01<00:00, 56.66it/s]2025-11-12 00:38:39,079 - model_processors - INFO - Reached max_tokens limit (100), stopping merge addition\n",
            "Adding merges & tokens:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 109/111 [00:01<00:00, 59.45it/s]\n",
            "2025-11-12 00:38:39,079 - model_processors - INFO - ‚úÖ Buffer 1.5x successful: added 100 tokens via 100 merges\n",
            "2025-11-12 00:38:39,079 - model_processors - INFO - Original vocab: 262,144, New vocab: 262,244 (+100)\n",
            "2025-11-12 00:38:39,079 - model_processors - INFO - Original merges: 514,906, New merges: 515,006 (+100)\n",
            "2025-11-12 00:38:41,783 - model_processors - INFO - ‚úÖ Successfully expanded vocabulary using train_new_from_iterator\n",
            "2025-11-12 00:38:41,784 - model_processors - INFO - Original vocabulary size: 262,145\n",
            "2025-11-12 00:38:41,785 - model_processors - INFO - New vocabulary size: 262,245\n",
            "2025-11-12 00:38:41,786 - model_processors - INFO - Actual tokens added: 100\n",
            "2025-11-12 00:38:41,786 - model_processors - INFO - Sample new tokens: ['‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã', '‡Ωº‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω¥', '‡Ω£‡æ∑‡Ωº‡ºã‡Ω¢‡æí‡æ±‡ºã‡ΩÇ‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã', '‡ºã‡ΩÇ‡Ωì‡Ω¶‡ºã', '‡ΩÄ‡æ±', '‡Ω¢‡Ω≤‡ΩÇ', '‡Ωö‡Ωº', '‡Ωß‡ºã‡ΩÖ', '‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ', '‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç']...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE\n",
            "================================================================================\n",
            "‚úÖ Tokens added: 100\n",
            "‚úÖ Original vocabulary size: 262,145\n",
            "‚úÖ New vocabulary size: 262,245\n",
            "‚úÖ Vocabulary increase: 100\n",
            "\n",
            "Sample new tokens: ['‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã', '‡Ωº‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω¥', '‡Ω£‡æ∑‡Ωº‡ºã‡Ω¢‡æí‡æ±‡ºã‡ΩÇ‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã', '‡ºã‡ΩÇ‡Ωì‡Ω¶‡ºã', '‡ΩÄ‡æ±', '‡Ω¢‡Ω≤‡ΩÇ', '‡Ωö‡Ωº', '‡Ωß‡ºã‡ΩÖ', '‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ', '‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç']...\n"
          ]
        }
      ],
      "source": [
        "# Expand tokenizer with Tibetan corpus\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING TOKENIZER\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTraining on {len(TIBETAN_CORPUS)} Tibetan corpus samples...\")\n",
        "print(f\"Target: Add 100 new tokens\\n\")\n",
        "\n",
        "expanded_tokenizer, tokens_added, new_tokens = processor.expand_tokenizer(\n",
        "    algorithm_name=\"SENTENCEPIECE_BPE\",\n",
        "    max_tokens=100,\n",
        "    training_corpus=TIBETAN_CORPUS\n",
        ")\n",
        "\n",
        "new_vocab_size = len(expanded_tokenizer.get_vocab())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úÖ Tokens added: {tokens_added}\")\n",
        "print(f\"‚úÖ Original vocabulary size: {original_vocab_size:,}\")\n",
        "print(f\"‚úÖ New vocabulary size: {new_vocab_size:,}\")\n",
        "print(f\"‚úÖ Vocabulary increase: {new_vocab_size - original_vocab_size:,}\")\n",
        "if new_tokens:\n",
        "    print(f\"\\nSample new tokens: {new_tokens[:10]}{'...' if len(new_tokens) > 10 else ''}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Tokenization AFTER Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "AFTER TRAINING\n",
            "================================================================================\n",
            "\n",
            "Sample text: ‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω¢‡æí‡æ±‡Ω£‡ºã‡ΩÅ‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ωë‡Ω∫‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\n",
            "\n",
            "Token IDs: [2, 262144, 242070, 212637, 262153, 203431, 100206, 242268, 155394, 242079, 151310, 262156, 242070, 212637, 240451, 262168, 236743, 52855, 242545, 235862, 239938, 120295, 262167, 239985, 155394, 239935, 262159, 92089, 157435, 201958, 262177]\n",
            "\n",
            "Number of tokens: 31\n",
            "\n",
            "Token strings:\n",
            "  [0] ID:      2 | Token: '<bos>'\n",
            "  [1] ID: 262144 | Token: '‡Ωñ‡Ωº‡Ωë‡ºã'\n",
            "  [2] ID: 242070 | Token: '‡Ω°'\n",
            "  [3] ID: 212637 | Token: '‡Ω≤‡ΩÇ‡ºã'\n",
            "  [4] ID: 262153 | Token: '‡Ωì‡Ω≤‡ºã'\n",
            "  [5] ID: 203431 | Token: '‡Ω¢‡æí‡æ±'\n",
            "  [6] ID: 100206 | Token: '‡Ω£‡ºã'\n",
            "  [7] ID: 242268 | Token: '‡ΩÅ'\n",
            "  [8] ID: 155394 | Token: '‡Ωñ‡ºã'\n",
            "  [9] ID: 242079 | Token: '‡ΩÄ'\n",
            "  [10] ID: 151310 | Token: '‡æ±‡Ω≤‡ºã'\n",
            "  [11] ID: 262156 | Token: '‡Ω¶‡æê‡Ωë‡ºã'\n",
            "  [12] ID: 242070 | Token: '‡Ω°'\n",
            "  [13] ID: 212637 | Token: '‡Ω≤‡ΩÇ‡ºã'\n",
            "  [14] ID: 240451 | Token: '‡Ω¢'\n",
            "  [15] ID: 262168 | Token: '‡Ω∫‡Ωë‡ºç'\n",
            "  [16] ID: 236743 | Token: '‚ñÅ'\n",
            "  [17] ID:  52855 | Token: '‡ΩÑ‡ºã'\n",
            "  [18] ID: 242545 | Token: '‡Ωö'\n",
            "  [19] ID: 235862 | Token: '‡Ωº‡Ω¶‡ºã'\n",
            "  [20] ID: 239938 | Token: '‡Ωë'\n",
            "  [21] ID: 120295 | Token: '‡Ω∫‡ºã'\n",
            "  [22] ID: 262167 | Token: '‡Ω¶‡æ≥'\n",
            "  [23] ID: 239985 | Token: '‡Ωº'\n",
            "  [24] ID: 155394 | Token: '‡Ωñ‡ºã'\n",
            "  [25] ID: 239935 | Token: '‡ΩÇ'\n",
            "  [26] ID: 262159 | Token: '‡Ωâ‡Ω∫'\n",
            "  [27] ID:  92089 | Token: '‡Ω¢‡ºã'\n",
            "  [28] ID: 157435 | Token: '‡Ωñ‡æ±'\n",
            "  [29] ID: 201958 | Token: '‡Ω∫‡Ωë‡ºã'\n",
            "  [30] ID: 262177 | Token: '‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç'\n",
            "\n",
            "Vocabulary size: 262,245\n"
          ]
        }
      ],
      "source": [
        "# Encode same sample text with expanded tokenizer\n",
        "after_token_ids = expanded_tokenizer.encode(SAMPLE_TIBETAN_TEXT)\n",
        "after_tokens = expanded_tokenizer.convert_ids_to_tokens(after_token_ids)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AFTER TRAINING\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nSample text: {SAMPLE_TIBETAN_TEXT}\")\n",
        "print(f\"\\nToken IDs: {after_token_ids}\")\n",
        "print(f\"\\nNumber of tokens: {len(after_token_ids)}\")\n",
        "print(f\"\\nToken strings:\")\n",
        "for i, (token_id, token) in enumerate(zip(after_token_ids, after_tokens)):\n",
        "    print(f\"  [{i}] ID: {token_id:6d} | Token: '{token}'\")\n",
        "print(f\"\\nVocabulary size: {new_vocab_size:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.5. Side-by-Side Encode Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ENCODE() FUNCTION COMPARISON\n",
            "================================================================================\n",
            "\n",
            "Sample text: ‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω¢‡æí‡æ±‡Ω£‡ºã‡ΩÅ‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ωë‡Ω∫‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\n",
            "\n",
            "BEFORE (Original Tokenizer):\n",
            "  encode() result: ['<bos>', '‡Ωñ', '‡Ωº‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ωì', '‡Ω≤‡ºã', '‡Ω¢‡æí‡æ±', '‡Ω£‡ºã', '‡ΩÅ', '‡Ωñ‡ºã', '‡ΩÄ', '‡æ±‡Ω≤‡ºã', '‡Ω¶‡æê', '‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ω¢', '‡Ω∫', '‡Ωë', '‡ºç', '‚ñÅ', '‡ΩÑ‡ºã', '‡Ωö', '‡Ωº‡Ω¶‡ºã', '‡Ωë', '‡Ω∫‡ºã', '‡Ω¶', '‡æ≥', '‡Ωº', '‡Ωñ‡ºã', '‡ΩÇ', '‡Ωâ', '‡Ω∫', '‡Ω¢‡ºã', '‡Ωñ‡æ±', '‡Ω∫‡Ωë‡ºã', '‡Ωë', '‡ΩÇ', '‡Ωº', '‡Ω¶', '‡ºç']\n",
            "  Length: 42 tokens\n",
            "\n",
            "AFTER (Expanded Tokenizer):\n",
            "  encode() result: ['<bos>', '‡Ωñ‡Ωº‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ωì‡Ω≤‡ºã', '‡Ω¢‡æí‡æ±', '‡Ω£‡ºã', '‡ΩÅ', '‡Ωñ‡ºã', '‡ΩÄ', '‡æ±‡Ω≤‡ºã', '‡Ω¶‡æê‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ω¢', '‡Ω∫‡Ωë‡ºç', '‚ñÅ', '‡ΩÑ‡ºã', '‡Ωö', '‡Ωº‡Ω¶‡ºã', '‡Ωë', '‡Ω∫‡ºã', '‡Ω¶‡æ≥', '‡Ωº', '‡Ωñ‡ºã', '‡ΩÇ', '‡Ωâ‡Ω∫', '‡Ω¢‡ºã', '‡Ωñ‡æ±', '‡Ω∫‡Ωë‡ºã', '‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç']\n",
            "  Length: 31 tokens\n",
            "\n",
            "================================================================================\n",
            "DIFFERENCE:\n",
            "================================================================================\n",
            "  Token count changed: 42 ‚Üí 31 (-11)\n",
            "  Token IDs changed: Different tokenization\n",
            "\n",
            "  Positions with different token IDs:\n",
            "    Position 1: 240332 ‚Üí 262144\n",
            "    Position 2: 144766 ‚Üí 242070\n",
            "    Position 3: 242070 ‚Üí 212637\n",
            "    Position 4: 212637 ‚Üí 262153\n",
            "    Position 5: 240392 ‚Üí 203431\n",
            "    Position 6: 55765 ‚Üí 100206\n",
            "    Position 7: 203431 ‚Üí 242268\n",
            "    Position 8: 100206 ‚Üí 155394\n",
            "    Position 9: 242268 ‚Üí 242079\n",
            "    Position 10: 155394 ‚Üí 151310\n",
            "    ... and 31 more differences\n"
          ]
        }
      ],
      "source": [
        "# Show encode() results side-by-side\n",
        "print(\"=\" * 80)\n",
        "print(\"ENCODE() FUNCTION COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nSample text: {SAMPLE_TIBETAN_TEXT}\\n\")\n",
        "\n",
        "# Get encode results\n",
        "before_encode = original_tokenizer.encode(SAMPLE_TIBETAN_TEXT)\n",
        "after_encode = expanded_tokenizer.encode(SAMPLE_TIBETAN_TEXT)\n",
        "\n",
        "print(\"BEFORE (Original Tokenizer):\")\n",
        "print(f\"  encode() result: {original_tokenizer.convert_ids_to_tokens(before_encode)}\")\n",
        "print(f\"  Length: {len(before_encode)} tokens\\n\")\n",
        "\n",
        "print(\"AFTER (Expanded Tokenizer):\")\n",
        "print(f\"  encode() result: {expanded_tokenizer.convert_ids_to_tokens(after_encode)}\")\n",
        "print(f\"  Length: {len(after_encode)} tokens\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DIFFERENCE:\")\n",
        "print(\"=\" * 80)\n",
        "if len(before_encode) != len(after_encode):\n",
        "    print(f\"  Token count changed: {len(before_encode)} ‚Üí {len(after_encode)} ({len(after_encode) - len(before_encode):+d})\")\n",
        "else:\n",
        "    print(f\"  Token count: Same ({len(before_encode)} tokens)\")\n",
        "\n",
        "if before_encode != after_encode:\n",
        "    print(f\"  Token IDs changed: Different tokenization\")\n",
        "    # Show which positions differ\n",
        "    max_len = max(len(before_encode), len(after_encode))\n",
        "    differences = []\n",
        "    for i in range(max_len):\n",
        "        before_val = before_encode[i] if i < len(before_encode) else None\n",
        "        after_val = after_encode[i] if i < len(after_encode) else None\n",
        "        if before_val != after_val:\n",
        "            differences.append((i, before_val, after_val))\n",
        "    \n",
        "    if differences:\n",
        "        print(f\"\\n  Positions with different token IDs:\")\n",
        "        for pos, before_val, after_val in differences[:10]:  # Show first 10 differences\n",
        "            print(f\"    Position {pos}: {before_val} ‚Üí {after_val}\")\n",
        "        if len(differences) > 10:\n",
        "            print(f\"    ... and {len(differences) - 10} more differences\")\n",
        "else:\n",
        "    print(f\"  Token IDs: Identical\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Tokenizer with 1000 Tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TRAINING TOKENIZER WITH 1000 TOKENS\n",
            "================================================================================\n",
            "\n",
            "Training on 500 Tibetan corpus samples...\n",
            "Target: Add 1000 new tokens\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 00:38:44,746 - model_processors - INFO - \n",
            "=== EXPANDING GOOGLE/GEMMA-3-1B-IT WITH SENTENCEPIECE_BPE ===\n",
            "2025-11-12 00:38:44,746 - model_processors - INFO - Original vocabulary size: 262,145\n",
            "2025-11-12 00:38:44,746 - model_processors - INFO - Training corpus size: 500\n",
            "2025-11-12 00:38:44,762 - model_processors - INFO - Using train_new_from_iterator approach for SENTENCEPIECE_BPE with 500 corpus samples\n",
            "2025-11-12 00:38:44,763 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "2025-11-12 00:38:47,562 - model_processors - INFO - Base tokenizer model type: BPE\n",
            "2025-11-12 00:38:47,562 - model_processors - INFO - New tokenizer model type: BPE\n",
            "2025-11-12 00:38:47,562 - model_processors - INFO - Trying 1.5x buffer: target vocab size = 1,500 (1500 new tokens to select 1000 best from)\n",
            "2025-11-12 00:38:47,562 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "Adding merges & tokens: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:02<00:00, 58.23it/s]\n",
            "2025-11-12 00:38:52,442 - model_processors - INFO - ‚ùå Buffer 1.5x insufficient: only 113 tokens (need 1000), trying next buffer...\n",
            "2025-11-12 00:38:52,443 - model_processors - INFO - Trying 2x buffer: target vocab size = 2,000 (2000 new tokens to select 1000 best from)\n",
            "2025-11-12 00:38:52,445 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "Adding merges & tokens: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:02<00:00, 56.34it/s]\n",
            "2025-11-12 00:38:57,360 - model_processors - INFO - ‚ùå Buffer 2x insufficient: only 113 tokens (need 1000), trying next buffer...\n",
            "2025-11-12 00:38:57,360 - model_processors - INFO - Trying 3x buffer: target vocab size = 3,000 (3000 new tokens to select 1000 best from)\n",
            "2025-11-12 00:38:57,360 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "Adding merges & tokens: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:02<00:00, 57.69it/s]\n",
            "2025-11-12 00:39:02,750 - model_processors - INFO - ‚ùå Buffer 3x insufficient: only 113 tokens (need 1000), trying next buffer...\n",
            "2025-11-12 00:39:02,750 - model_processors - INFO - Trying 4x buffer: target vocab size = 4,000 (4000 new tokens to select 1000 best from)\n",
            "2025-11-12 00:39:02,750 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "Adding merges & tokens: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:02<00:00, 58.05it/s]\n",
            "2025-11-12 00:39:07,603 - model_processors - INFO - ‚ùå Buffer 4x insufficient: only 113 tokens (need 1000), trying next buffer...\n",
            "2025-11-12 00:39:07,603 - model_processors - INFO - Trying 5x buffer: target vocab size = 5,000 (5000 new tokens to select 1000 best from)\n",
            "2025-11-12 00:39:07,603 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "Adding merges & tokens: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:02<00:00, 57.61it/s]\n",
            "2025-11-12 00:39:12,451 - model_processors - INFO - ‚ùå Buffer 5x insufficient: only 113 tokens (need 1000), trying next buffer...\n",
            "2025-11-12 00:39:12,451 - model_processors - INFO - Trying 8x buffer: target vocab size = 8,000 (8000 new tokens to select 1000 best from)\n",
            "2025-11-12 00:39:12,451 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "Adding merges & tokens: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:02<00:00, 57.28it/s]\n",
            "2025-11-12 00:39:17,200 - model_processors - INFO - ‚ùå Buffer 8x insufficient: only 113 tokens (need 1000), trying next buffer...\n",
            "2025-11-12 00:39:17,201 - model_processors - INFO - Trying 12x buffer: target vocab size = 12,000 (12000 new tokens to select 1000 best from)\n",
            "2025-11-12 00:39:17,202 - model_processors - INFO - Training new tokenizer from base using corpus...\n",
            "Adding merges & tokens: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:02<00:00, 58.07it/s]\n",
            "2025-11-12 00:39:22,609 - model_processors - WARNING - ‚ö†Ô∏è  Could not reach target of 1000 tokens even with largest buffer (12x). Adding 113 tokens (best available). The corpus may be too small or not diverse enough.\n",
            "2025-11-12 00:39:22,609 - model_processors - INFO - Applied 12x buffer result: added 113 tokens via 113 merges\n",
            "2025-11-12 00:39:22,609 - model_processors - INFO - Original vocab: 262,144, New vocab: 262,257 (+113)\n",
            "2025-11-12 00:39:22,609 - model_processors - INFO - Original merges: 514,906, New merges: 515,019 (+113)\n",
            "2025-11-12 00:39:25,258 - model_processors - INFO - ‚úÖ Successfully expanded vocabulary using train_new_from_iterator\n",
            "2025-11-12 00:39:25,258 - model_processors - INFO - Original vocabulary size: 262,145\n",
            "2025-11-12 00:39:25,258 - model_processors - INFO - New vocabulary size: 262,258\n",
            "2025-11-12 00:39:25,258 - model_processors - INFO - Actual tokens added: 113\n",
            "2025-11-12 00:39:25,258 - model_processors - INFO - Sample new tokens: ['‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã', '‡Ωº‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω¥', '‡Ω£‡æ∑‡Ωº‡ºã‡Ω¢‡æí‡æ±‡ºã‡ΩÇ‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã', '‡ºã‡ΩÇ‡Ωì‡Ω¶‡ºã', '‡ΩÄ‡æ±', '‡Ω¢‡Ω≤‡ΩÇ', '‡Ωö‡Ωº', '‡Ωß‡ºã‡ΩÖ', '‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ', '‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç']...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE (1000 tokens)\n",
            "================================================================================\n",
            "‚úÖ Tokens added: 113\n",
            "‚úÖ Original vocabulary size: 262,145\n",
            "‚úÖ New vocabulary size: 262,258\n",
            "‚úÖ Vocabulary increase: 113\n",
            "\n",
            "Sample new tokens: ['‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã', '‡Ωº‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω¥', '‡Ω£‡æ∑‡Ωº‡ºã‡Ω¢‡æí‡æ±‡ºã‡ΩÇ‡Ω¢‡ºã‡Ωë‡ΩÑ‡ºã', '‡ºã‡ΩÇ‡Ωì‡Ω¶‡ºã', '‡ΩÄ‡æ±', '‡Ω¢‡Ω≤‡ΩÇ', '‡Ωö‡Ωº', '‡Ωß‡ºã‡ΩÖ', '‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ', '‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç']...\n"
          ]
        }
      ],
      "source": [
        "# Expand tokenizer with Tibetan corpus - 1000 tokens\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING TOKENIZER WITH 1000 TOKENS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTraining on {len(TIBETAN_CORPUS)} Tibetan corpus samples...\")\n",
        "print(f\"Target: Add 1000 new tokens\\n\")\n",
        "\n",
        "expanded_tokenizer_1000, tokens_added_1000, new_tokens_1000 = processor.expand_tokenizer(\n",
        "    algorithm_name=\"SENTENCEPIECE_BPE\",\n",
        "    max_tokens=1000,\n",
        "    training_corpus=TIBETAN_CORPUS\n",
        ")\n",
        "\n",
        "new_vocab_size_1000 = len(expanded_tokenizer_1000.get_vocab())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING COMPLETE (1000 tokens)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úÖ Tokens added: {tokens_added_1000}\")\n",
        "print(f\"‚úÖ Original vocabulary size: {original_vocab_size:,}\")\n",
        "print(f\"‚úÖ New vocabulary size: {new_vocab_size_1000:,}\")\n",
        "print(f\"‚úÖ Vocabulary increase: {new_vocab_size_1000 - original_vocab_size:,}\")\n",
        "if new_tokens_1000:\n",
        "    print(f\"\\nSample new tokens: {new_tokens_1000[:10]}{'...' if len(new_tokens_1000) > 10 else ''}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Tokenization with 1000-Token Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "AFTER TRAINING (1000 tokens)\n",
            "================================================================================\n",
            "\n",
            "Sample text: ‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω¢‡æí‡æ±‡Ω£‡ºã‡ΩÅ‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ωë‡Ω∫‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\n",
            "\n",
            "Token IDs: [2, 262144, 242070, 212637, 262153, 203431, 100206, 242268, 155394, 242079, 151310, 262156, 242070, 212637, 240451, 262168, 236743, 52855, 242545, 235862, 239938, 120295, 262167, 239985, 155394, 239935, 262159, 92089, 157435, 201958, 262177]\n",
            "\n",
            "Number of tokens: 31\n",
            "\n",
            "Token strings:\n",
            "  [0] ID:      2 | Token: '<bos>'\n",
            "  [1] ID: 262144 | Token: '‡Ωñ‡Ωº‡Ωë‡ºã'\n",
            "  [2] ID: 242070 | Token: '‡Ω°'\n",
            "  [3] ID: 212637 | Token: '‡Ω≤‡ΩÇ‡ºã'\n",
            "  [4] ID: 262153 | Token: '‡Ωì‡Ω≤‡ºã'\n",
            "  [5] ID: 203431 | Token: '‡Ω¢‡æí‡æ±'\n",
            "  [6] ID: 100206 | Token: '‡Ω£‡ºã'\n",
            "  [7] ID: 242268 | Token: '‡ΩÅ'\n",
            "  [8] ID: 155394 | Token: '‡Ωñ‡ºã'\n",
            "  [9] ID: 242079 | Token: '‡ΩÄ'\n",
            "  [10] ID: 151310 | Token: '‡æ±‡Ω≤‡ºã'\n",
            "  [11] ID: 262156 | Token: '‡Ω¶‡æê‡Ωë‡ºã'\n",
            "  [12] ID: 242070 | Token: '‡Ω°'\n",
            "  [13] ID: 212637 | Token: '‡Ω≤‡ΩÇ‡ºã'\n",
            "  [14] ID: 240451 | Token: '‡Ω¢'\n",
            "  [15] ID: 262168 | Token: '‡Ω∫‡Ωë‡ºç'\n",
            "  [16] ID: 236743 | Token: '‚ñÅ'\n",
            "  [17] ID:  52855 | Token: '‡ΩÑ‡ºã'\n",
            "  [18] ID: 242545 | Token: '‡Ωö'\n",
            "  [19] ID: 235862 | Token: '‡Ωº‡Ω¶‡ºã'\n",
            "  [20] ID: 239938 | Token: '‡Ωë'\n",
            "  [21] ID: 120295 | Token: '‡Ω∫‡ºã'\n",
            "  [22] ID: 262167 | Token: '‡Ω¶‡æ≥'\n",
            "  [23] ID: 239985 | Token: '‡Ωº'\n",
            "  [24] ID: 155394 | Token: '‡Ωñ‡ºã'\n",
            "  [25] ID: 239935 | Token: '‡ΩÇ'\n",
            "  [26] ID: 262159 | Token: '‡Ωâ‡Ω∫'\n",
            "  [27] ID:  92089 | Token: '‡Ω¢‡ºã'\n",
            "  [28] ID: 157435 | Token: '‡Ωñ‡æ±'\n",
            "  [29] ID: 201958 | Token: '‡Ω∫‡Ωë‡ºã'\n",
            "  [30] ID: 262177 | Token: '‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç'\n",
            "\n",
            "Vocabulary size: 262,258\n"
          ]
        }
      ],
      "source": [
        "# Encode same sample text with 1000-token expanded tokenizer\n",
        "after_token_ids_1000 = expanded_tokenizer_1000.encode(SAMPLE_TIBETAN_TEXT)\n",
        "after_tokens_1000 = expanded_tokenizer_1000.convert_ids_to_tokens(after_token_ids_1000)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AFTER TRAINING (1000 tokens)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nSample text: {SAMPLE_TIBETAN_TEXT}\")\n",
        "print(f\"\\nToken IDs: {after_token_ids_1000}\")\n",
        "print(f\"\\nNumber of tokens: {len(after_token_ids_1000)}\")\n",
        "print(f\"\\nToken strings:\")\n",
        "for i, (token_id, token) in enumerate(zip(after_token_ids_1000, after_tokens_1000)):\n",
        "    print(f\"  [{i}] ID: {token_id:6d} | Token: '{token}'\")\n",
        "print(f\"\\nVocabulary size: {new_vocab_size_1000:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Encode Comparison: All Three Tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ENCODE() FUNCTION COMPARISON - ALL THREE TOKENIZERS\n",
            "================================================================================\n",
            "\n",
            "Sample text: ‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω¢‡æí‡æ±‡Ω£‡ºã‡ΩÅ‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω¶‡ºã‡Ωë‡Ω∫‡ºã‡Ω¶‡æ≥‡Ωº‡Ωñ‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ω¢‡ºã‡Ωñ‡æ±‡Ω∫‡Ωë‡ºã‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç\n",
            "\n",
            "ORIGINAL (Before Training):\n",
            "  encode() result: ['<bos>', '‡Ωñ', '‡Ωº‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ωì', '‡Ω≤‡ºã', '‡Ω¢‡æí‡æ±', '‡Ω£‡ºã', '‡ΩÅ', '‡Ωñ‡ºã', '‡ΩÄ', '‡æ±‡Ω≤‡ºã', '‡Ω¶‡æê', '‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ω¢', '‡Ω∫', '‡Ωë', '‡ºç', '‚ñÅ', '‡ΩÑ‡ºã', '‡Ωö', '‡Ωº‡Ω¶‡ºã', '‡Ωë', '‡Ω∫‡ºã', '‡Ω¶', '‡æ≥', '‡Ωº', '‡Ωñ‡ºã', '‡ΩÇ', '‡Ωâ', '‡Ω∫', '‡Ω¢‡ºã', '‡Ωñ‡æ±', '‡Ω∫‡Ωë‡ºã', '‡Ωë', '‡ΩÇ', '‡Ωº', '‡Ω¶', '‡ºç']\n",
            "  Length: 42 tokens\n",
            "\n",
            "EXPANDED (100 tokens):\n",
            "  encode() result: ['<bos>', '‡Ωñ‡Ωº‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ωì‡Ω≤‡ºã', '‡Ω¢‡æí‡æ±', '‡Ω£‡ºã', '‡ΩÅ', '‡Ωñ‡ºã', '‡ΩÄ', '‡æ±‡Ω≤‡ºã', '‡Ω¶‡æê‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ω¢', '‡Ω∫‡Ωë‡ºç', '‚ñÅ', '‡ΩÑ‡ºã', '‡Ωö', '‡Ωº‡Ω¶‡ºã', '‡Ωë', '‡Ω∫‡ºã', '‡Ω¶‡æ≥', '‡Ωº', '‡Ωñ‡ºã', '‡ΩÇ', '‡Ωâ‡Ω∫', '‡Ω¢‡ºã', '‡Ωñ‡æ±', '‡Ω∫‡Ωë‡ºã', '‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç']\n",
            "  Length: 31 tokens\n",
            "\n",
            "EXPANDED (1000 tokens):\n",
            "  encode() result: ['<bos>', '‡Ωñ‡Ωº‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ωì‡Ω≤‡ºã', '‡Ω¢‡æí‡æ±', '‡Ω£‡ºã', '‡ΩÅ', '‡Ωñ‡ºã', '‡ΩÄ', '‡æ±‡Ω≤‡ºã', '‡Ω¶‡æê‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ω¢', '‡Ω∫‡Ωë‡ºç', '‚ñÅ', '‡ΩÑ‡ºã', '‡Ωö', '‡Ωº‡Ω¶‡ºã', '‡Ωë', '‡Ω∫‡ºã', '‡Ω¶‡æ≥', '‡Ωº', '‡Ωñ‡ºã', '‡ΩÇ', '‡Ωâ‡Ω∫', '‡Ω¢‡ºã', '‡Ωñ‡æ±', '‡Ω∫‡Ωë‡ºã', '‡Ωë‡ΩÇ‡Ωº‡Ω¶‡ºç']\n",
            "  Length: 31 tokens\n",
            "\n",
            "================================================================================\n",
            "SUMMARY:\n",
            "================================================================================\n",
            "  Original:   42 tokens\n",
            "  100 tokens:  31 tokens (-11 change)\n",
            "  1000 tokens:  31 tokens (-11 change)\n",
            "\n",
            "  Best (fewest tokens): 1000-token tokenizer\n"
          ]
        }
      ],
      "source": [
        "# Show encode() results for all three tokenizers\n",
        "print(\"=\" * 80)\n",
        "print(\"ENCODE() FUNCTION COMPARISON - ALL THREE TOKENIZERS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nSample text: {SAMPLE_TIBETAN_TEXT}\\n\")\n",
        "\n",
        "# Get encode results\n",
        "before_encode = original_tokenizer.encode(SAMPLE_TIBETAN_TEXT)\n",
        "after_encode_100 = expanded_tokenizer.encode(SAMPLE_TIBETAN_TEXT)\n",
        "after_encode_1000 = expanded_tokenizer_1000.encode(SAMPLE_TIBETAN_TEXT)\n",
        "\n",
        "print(\"ORIGINAL (Before Training):\")\n",
        "print(f\"  encode() result: {original_tokenizer.convert_ids_to_tokens(before_encode)}\")\n",
        "print(f\"  Length: {len(before_encode)} tokens\\n\")\n",
        "\n",
        "print(\"EXPANDED (100 tokens):\")\n",
        "print(f\"  encode() result: {expanded_tokenizer.convert_ids_to_tokens(after_encode_100)}\")\n",
        "print(f\"  Length: {len(after_encode_100)} tokens\\n\")\n",
        "\n",
        "print(\"EXPANDED (1000 tokens):\")\n",
        "print(f\"  encode() result: {expanded_tokenizer_1000.convert_ids_to_tokens(after_encode_1000)}\")\n",
        "print(f\"  Length: {len(after_encode_1000)} tokens\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"  Original:  {len(before_encode):3d} tokens\")\n",
        "print(f\"  100 tokens: {len(after_encode_100):3d} tokens ({len(after_encode_100) - len(before_encode):+d} change)\")\n",
        "print(f\"  1000 tokens: {len(after_encode_1000):3d} tokens ({len(after_encode_1000) - len(before_encode):+d} change)\")\n",
        "print(f\"\\n  Best (fewest tokens): {'1000-token' if len(after_encode_1000) <= len(after_encode_100) else '100-token' if len(after_encode_100) < len(before_encode) else 'Original'} tokenizer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Complete Comparison: All Three Tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "COMPLETE COMPARISON: ORIGINAL vs 100 TOKENS vs 113 TOKENS\n",
            "================================================================================\n",
            "\n",
            "Metric                         Original             100 tokens           113 tokens          \n",
            "------------------------------------------------------------------------------------------\n",
            "Vocabulary Size                           262,145            262,245            262,258\n",
            "Tokens Added                                  N/A                100                113\n",
            "Number of Tokens                               42                 31                 31\n",
            "\n",
            "================================================================================\n",
            "TOKEN COMPARISON\n",
            "================================================================================\n",
            "\n",
            "Original (42 tokens):\n",
            "  IDs: [2, 240332, 144766, 242070, 212637, 240392, 55765, 203431, 100206, 242268, 155394, 242079, 151310, 202660, 73436, 242070, 212637, 240451, 240640, 239938, 241594, 236743, 52855, 242545, 235862, 239938, 120295, 239502, 243797, 239985, 155394, 239935, 243835, 240640, 92089, 157435, 201958, 239938, 239935, 239985, 239502, 241594]\n",
            "  Tokens: ['<bos>', '‡Ωñ', '‡Ωº‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã']...\n",
            "\n",
            "100 tokens (31 tokens):\n",
            "  IDs: [2, 262144, 242070, 212637, 262153, 203431, 100206, 242268, 155394, 242079, 151310, 262156, 242070, 212637, 240451, 262168, 236743, 52855, 242545, 235862, 239938, 120295, 262167, 239985, 155394, 239935, 262159, 92089, 157435, 201958, 262177]\n",
            "  Tokens: ['<bos>', '‡Ωñ‡Ωº‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ωì‡Ω≤‡ºã']...\n",
            "\n",
            "1000 tokens (31 tokens):\n",
            "  IDs: [2, 262144, 242070, 212637, 262153, 203431, 100206, 242268, 155394, 242079, 151310, 262156, 242070, 212637, 240451, 262168, 236743, 52855, 242545, 235862, 239938, 120295, 262167, 239985, 155394, 239935, 262159, 92089, 157435, 201958, 262177]\n",
            "  Tokens: ['<bos>', '‡Ωñ‡Ωº‡Ωë‡ºã', '‡Ω°', '‡Ω≤‡ΩÇ‡ºã', '‡Ωì‡Ω≤‡ºã']...\n",
            "\n",
            "================================================================================\n",
            "IMPROVEMENT ANALYSIS\n",
            "================================================================================\n",
            "‚úÖ 100 tokens: 11 fewer tokens (26.2% reduction)\n",
            "‚úÖ 1000 tokens: 11 fewer tokens (26.2% reduction)\n",
            "\n",
            "üí° Both expanded tokenizers provide similar compression\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"COMPLETE COMPARISON: ORIGINAL vs 100 TOKENS vs 113 TOKENS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n{'Metric':<30} {'Original':<20} {'100 tokens':<20} {'113 tokens':<20}\")\n",
        "print(\"-\" * 90)\n",
        "print(f\"{'Vocabulary Size':<30} {original_vocab_size:>18,} {new_vocab_size:>18,} {new_vocab_size_1000:>18,}\")\n",
        "print(f\"{'Tokens Added':<30} {'N/A':>18} {tokens_added:>18} {tokens_added_1000:>18}\")\n",
        "print(f\"{'Number of Tokens':<30} {len(before_token_ids):>18} {len(after_token_ids):>18} {len(after_token_ids_1000):>18}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TOKEN COMPARISON\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"\\nOriginal ({len(before_token_ids)} tokens):\")\n",
        "print(f\"  IDs: {before_token_ids}\")\n",
        "print(f\"  Tokens: {before_tokens[:5]}{'...' if len(before_tokens) > 5 else ''}\")\n",
        "\n",
        "print(f\"\\n100 tokens ({len(after_token_ids)} tokens):\")\n",
        "print(f\"  IDs: {after_token_ids}\")\n",
        "print(f\"  Tokens: {after_tokens[:5]}{'...' if len(after_tokens) > 5 else ''}\")\n",
        "\n",
        "print(f\"\\n1000 tokens ({len(after_token_ids_1000)} tokens):\")\n",
        "print(f\"  IDs: {after_token_ids_1000}\")\n",
        "print(f\"  Tokens: {after_tokens_1000[:5]}{'...' if len(after_tokens_1000) > 5 else ''}\")\n",
        "\n",
        "# Calculate improvements\n",
        "improvement_100 = len(before_token_ids) - len(after_token_ids)\n",
        "improvement_1000 = len(before_token_ids) - len(after_token_ids_1000)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"IMPROVEMENT ANALYSIS\")\n",
        "print(f\"{'='*80}\")\n",
        "if improvement_100 > 0:\n",
        "    reduction_pct_100 = (improvement_100 / len(before_token_ids)) * 100\n",
        "    print(f\"‚úÖ 100 tokens: {improvement_100} fewer tokens ({reduction_pct_100:.1f}% reduction)\")\n",
        "elif improvement_100 == 0:\n",
        "    print(f\"‚û°Ô∏è  100 tokens: Same number of tokens\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  100 tokens: {abs(improvement_100)} more tokens\")\n",
        "\n",
        "if improvement_1000 > 0:\n",
        "    reduction_pct_1000 = (improvement_1000 / len(before_token_ids)) * 100\n",
        "    print(f\"‚úÖ 1000 tokens: {improvement_1000} fewer tokens ({reduction_pct_1000:.1f}% reduction)\")\n",
        "elif improvement_1000 == 0:\n",
        "    print(f\"‚û°Ô∏è  113 tokens: Same number of tokens\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  1000 tokens: {abs(improvement_1000)} more tokens\")\n",
        "\n",
        "if improvement_1000 > improvement_100:\n",
        "    print(f\"\\nüí° 1000-token tokenizer provides better compression than 100-token tokenizer\")\n",
        "elif improvement_1000 < improvement_100:\n",
        "    print(f\"\\nüí° 100-token tokenizer provides better compression than 113-token tokenizer\")\n",
        "else:\n",
        "    print(f\"\\nüí° Both expanded tokenizers provide similar compression\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
