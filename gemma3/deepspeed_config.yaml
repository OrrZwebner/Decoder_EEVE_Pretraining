# DeepSpeed Configuration (YAML format)
train_batch_size: auto
train_micro_batch_size_per_gpu: auto
gradient_accumulation_steps: auto
gradient_clipping: 1.0

zero_optimization:
  stage: 2
  # Optional: Add more ZeRO-2 settings
  # allgather_partitions: true
  # allgather_bucket_size: 2e8
  # reduce_scatter: true
  # reduce_bucket_size: 2e8
  # overlap_comm: false
  # contiguous_gradients: true

# Precision settings - use auto to match training args
fp16:
  enabled: auto

bf16:
  enabled: auto

# Optimizer (optional - can use auto)
# optimizer:
#   type: AdamW
#   params:
#     lr: auto
#     weight_decay: auto

# Scheduler (optional)
# scheduler:
#   type: WarmupLR
#   params:
#     warmup_num_steps: auto

# Logging
steps_per_print: 100
wall_clock_breakdown: false

# Optional: Tensorboard
# tensorboard:
#   enabled: false
#   output_path: ./tensorboard_logs/
#   job_name: gemma3_sanskrit