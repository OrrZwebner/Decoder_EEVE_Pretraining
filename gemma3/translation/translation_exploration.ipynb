{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Fine-tuning - Exploration & Testing\n",
    "\n",
    "This notebook provides:\n",
    "- Dataset exploration (sample NLLB-200 data)\n",
    "- Model testing (generate translations)\n",
    "- Quality analysis (compare base vs fine-tuned)\n",
    "- Visualization (loss curves, BLEU scores, etc.)\n",
    "\n",
    "**Usage**: Use this to:\n",
    "1. Explore NLLB-200 dataset before training\n",
    "2. Test fine-tuned models interactively\n",
    "3. Visualize training results\n",
    "4. Compare different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # Specify which GPU(s) to use\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "HF Cache: /home/orrz/gpufs/hf/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "HF_CACHE = \"/home/orrz/gpufs/hf/.cache/huggingface\"\n",
    "\n",
    "# Set environment\n",
    "os.environ['HF_HOME'] = HF_CACHE\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"HF Cache: {HF_CACHE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Exploration\n",
    "\n",
    "Sample and visualize NLLB-200 dataset (English-Hebrew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NLLB-200 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLLB-200 (English-Hebrew)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset structure:\n",
      "IterableDataset({\n",
      "    features: ['translation', 'laser_score'],\n",
      "    num_shards: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load English-Hebrew dataset from NLLB-200\n",
    "print(\"Loading NLLB-200 (English-Hebrew)...\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"allenai/nllb\",\n",
    "    \"eng_Latn-heb_Hebr\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# print(f\"✓ Dataset loaded: {len(dataset):,} examples\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 5 English-Hebrew Translation Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled 5 English-Hebrew pairs:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "  English: That is God's design. that is God's desire.\n",
      "  Hebrew:  זהו מעמדו של אלוהים, זוהי זהותו של אלוהים.\n",
      "  Length:  43 chars (EN) | 42 chars (HE)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "  English: Still, Pharaoh’s heart is hardened and he refuses.\n",
      "  Hebrew:  פרעה מקשה את לבו ומסרב, הוא דורש נס.\n",
      "  Length:  50 chars (EN) | 36 chars (HE)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "  English: As you know, God shows up in the most unexpected places.”\n",
      "  Hebrew:  \"אני מניח שאלוהים נוטה להופיע במקומות הכי פחות צפויים.\"\n",
      "  Length:  57 chars (EN) | 55 chars (HE)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "  English: Alas, he wants to be successful even in his adventure with God.\"\n",
      "  Hebrew:  אולם למרבה הצער הוא רוצה להצליח אפילו בהרפתקתו עם האלוהים.\n",
      "  Length:  64 chars (EN) | 58 chars (HE)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "  English: This is God’s promise to His children.\n",
      "  Hebrew:  זו לכאורה התחייבות של האב כלפי ילדיו.\n",
      "  Length:  38 chars (EN) | 37 chars (HE)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample 5 random examples\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Get 5 random indices\n",
    "num_samples = 5\n",
    "# random_indices = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "# Extract samples\n",
    "for sample in dataset.shuffle(seed=42).take(num_samples):\n",
    "    samples = [sample for sample in dataset.shuffle(seed=42).take(num_samples)]\n",
    "\n",
    "print(f\"\\nSampled {num_samples} English-Hebrew pairs:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    english = sample['translation']['eng_Latn']\n",
    "    hebrew = sample['translation']['heb_Hebr']\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  English: {english}\")\n",
    "    print(f\"  Hebrew:  {hebrew}\")\n",
    "    print(f\"  Length:  {len(english)} chars (EN) | {len(hebrew)} chars (HE)\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame for Better Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas DataFrame\n",
    "df_samples = pd.DataFrame([\n",
    "    {\n",
    "        'ID': i,\n",
    "        'English': sample['translation']['eng_Latn'],\n",
    "        'Hebrew': sample['translation']['heb_Hebr'],\n",
    "        'EN_Length': len(sample['translation']['eng_Latn']),\n",
    "        'HE_Length': len(sample['translation']['heb_Hebr'])\n",
    "    }\n",
    "    for i, sample in enumerate(samples, 1)\n",
    "])\n",
    "\n",
    "print(\"\\nDataFrame of samples:\")\n",
    "df_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Text Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot character lengths\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = df_samples['ID']\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, df_samples['EN_Length'], width, label='English', color='steelblue')\n",
    "ax.bar(x + width/2, df_samples['HE_Length'], width, label='Hebrew', color='coral')\n",
    "\n",
    "ax.set_xlabel('Sample ID')\n",
    "ax.set_ylabel('Character Length')\n",
    "ax.set_title('Text Length Comparison (English vs Hebrew)')\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample more examples for statistics\n",
    "sample_size = 1000\n",
    "print(f\"Analyzing {sample_size} random samples...\\n\")\n",
    "\n",
    "sampled_dataset = dataset.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "# Extract lengths\n",
    "en_lengths = [len(ex['translation']['eng_Latn']) for ex in sampled_dataset]\n",
    "he_lengths = [len(ex['translation']['heb_Hebr']) for ex in sampled_dataset]\n",
    "\n",
    "# Statistics\n",
    "stats_df = pd.DataFrame({\n",
    "    'Metric': ['Mean', 'Median', 'Std Dev', 'Min', 'Max'],\n",
    "    'English': [\n",
    "        f\"{pd.Series(en_lengths).mean():.1f}\",\n",
    "        f\"{pd.Series(en_lengths).median():.1f}\",\n",
    "        f\"{pd.Series(en_lengths).std():.1f}\",\n",
    "        f\"{min(en_lengths)}\",\n",
    "        f\"{max(en_lengths)}\"\n",
    "    ],\n",
    "    'Hebrew': [\n",
    "        f\"{pd.Series(he_lengths).mean():.1f}\",\n",
    "        f\"{pd.Series(he_lengths).median():.1f}\",\n",
    "        f\"{pd.Series(he_lengths).std():.1f}\",\n",
    "        f\"{min(he_lengths)}\",\n",
    "        f\"{max(he_lengths)}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Character Length Statistics:\")\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# English distribution\n",
    "axes[0].hist(en_lengths, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(pd.Series(en_lengths).mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0].axvline(pd.Series(en_lengths).median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "axes[0].set_xlabel('Character Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('English Text Length Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Hebrew distribution\n",
    "axes[1].hist(he_lengths, bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(pd.Series(he_lengths).mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[1].axvline(pd.Series(he_lengths).median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "axes[1].set_xlabel('Character Length')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Hebrew Text Length Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Model Inference\n",
    "\n",
    "Load a model and generate translations for the sampled examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THESE\n",
    "BASE_MODEL = \"google/gemma-3-1b-it\"  # Or path to your fine-tuned model\n",
    "LORA_ADAPTER = None  # Path to LoRA adapter, or None for base model\n",
    "# LORA_ADAPTER = \"/home/orrz/gpufs/projects/gemma3/outputs/translation/gemma3-1b_en-he_20241029/final_model\"\n",
    "\n",
    "print(f\"Loading model: {BASE_MODEL}\")\n",
    "if LORA_ADAPTER:\n",
    "    print(f\"With LoRA adapter: {LORA_ADAPTER}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Load LoRA adapter if specified\n",
    "if LORA_ADAPTER:\n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(model, LORA_ADAPTER)\n",
    "    print(\"✓ LoRA adapter loaded\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"✓ Model loaded on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, source_lang=\"English\", target_lang=\"Hebrew\", max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    Translate text using the loaded model\n",
    "    \n",
    "    Args:\n",
    "        text: Source text to translate\n",
    "        source_lang: Source language name\n",
    "        target_lang: Target language name\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Translated text\n",
    "    \"\"\"\n",
    "    # Create prompt (matches training format)\n",
    "    prompt = f\"Translate from {source_lang} to {target_lang}: {text}\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode (remove input prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the translation (after the prompt)\n",
    "    if prompt in generated_text:\n",
    "        translation = generated_text[len(prompt):].strip()\n",
    "    else:\n",
    "        translation = generated_text.strip()\n",
    "    \n",
    "    return translation\n",
    "\n",
    "print(\"✓ Translation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Translations for Sampled Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translations for the 5 sampled examples\n",
    "print(\"Generating translations...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "translations = []\n",
    "\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    english = sample['translation']['eng_Latn']\n",
    "    hebrew_reference = sample['translation']['heb_Hebr']\n",
    "    \n",
    "    # Generate translation\n",
    "    hebrew_generated = translate(english, \"English\", \"Hebrew\")\n",
    "    \n",
    "    translations.append({\n",
    "        'id': i,\n",
    "        'english': english,\n",
    "        'hebrew_reference': hebrew_reference,\n",
    "        'hebrew_generated': hebrew_generated\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  English:    {english}\")\n",
    "    print(f\"  Reference:  {hebrew_reference}\")\n",
    "    print(f\"  Generated:  {hebrew_generated}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n✓ Translations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Comparison DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for easy comparison\n",
    "df_translations = pd.DataFrame(translations)\n",
    "\n",
    "print(\"\\nTranslation Results:\")\n",
    "df_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Simple Similarity Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character-level overlap (rough quality indicator)\n",
    "def calculate_overlap(ref, gen):\n",
    "    \"\"\"Calculate character overlap percentage\"\"\"\n",
    "    ref_chars = set(ref)\n",
    "    gen_chars = set(gen)\n",
    "    overlap = len(ref_chars & gen_chars)\n",
    "    total = len(ref_chars | gen_chars)\n",
    "    return (overlap / total * 100) if total > 0 else 0\n",
    "\n",
    "# Calculate for each example\n",
    "df_translations['overlap_pct'] = df_translations.apply(\n",
    "    lambda row: calculate_overlap(row['hebrew_reference'], row['hebrew_generated']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nCharacter Overlap (rough quality indicator):\")\n",
    "print(df_translations[['id', 'overlap_pct']].to_string(index=False))\n",
    "print(f\"\\nAverage overlap: {df_translations['overlap_pct'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Testing\n",
    "\n",
    "Try translating your own examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom input\n",
    "test_text = \"Hello, how are you today?\"  # EDIT THIS\n",
    "\n",
    "print(f\"Translating: {test_text}\\n\")\n",
    "translation = translate(test_text, \"English\", \"Hebrew\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple sentences\n",
    "test_sentences = [\n",
    "    \"Good morning!\",\n",
    "    \"Thank you very much.\",\n",
    "    \"Where is the library?\",\n",
    "    \"I would like a cup of coffee.\",\n",
    "    \"The weather is beautiful today.\"\n",
    "]\n",
    "\n",
    "print(\"Translating multiple sentences...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    translation = translate(sentence, \"English\", \"Hebrew\")\n",
    "    print(f\"\\n{i}. EN: {sentence}\")\n",
    "    print(f\"   HE: {translation}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations\n",
    "\n",
    "Additional analysis and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Loss (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs (if available)\n",
    "# EDIT THIS PATH to point to your training output\n",
    "TRAINING_OUTPUT_DIR = None\n",
    "# TRAINING_OUTPUT_DIR = \"/home/orrz/gpufs/projects/gemma3/outputs/translation/gemma3-1b_en-he_20241029/\"\n",
    "\n",
    "if TRAINING_OUTPUT_DIR and os.path.exists(TRAINING_OUTPUT_DIR):\n",
    "    # Try to find trainer_state.json\n",
    "    trainer_state_path = os.path.join(TRAINING_OUTPUT_DIR, \"trainer_state.json\")\n",
    "    \n",
    "    if os.path.exists(trainer_state_path):\n",
    "        import json\n",
    "        \n",
    "        with open(trainer_state_path) as f:\n",
    "            trainer_state = json.load(f)\n",
    "        \n",
    "        # Extract loss history\n",
    "        log_history = trainer_state['log_history']\n",
    "        \n",
    "        train_loss = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "        eval_loss = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        if train_loss:\n",
    "            ax.plot(train_loss, marker='o', label='Training Loss', linewidth=2)\n",
    "        if eval_loss:\n",
    "            eval_steps = [entry['step'] for entry in log_history if 'eval_loss' in entry]\n",
    "            ax.plot(eval_steps, eval_loss, marker='s', label='Eval Loss', linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Training & Evaluation Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✓ Loaded training logs from: {TRAINING_OUTPUT_DIR}\")\n",
    "    else:\n",
    "        print(f\"⚠ trainer_state.json not found in: {TRAINING_OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"ℹ Set TRAINING_OUTPUT_DIR to visualize training logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Text Length Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length ratios (Hebrew/English)\n",
    "if 'df_translations' in locals():\n",
    "    df_translations['ref_length'] = df_translations['hebrew_reference'].str.len()\n",
    "    df_translations['gen_length'] = df_translations['hebrew_generated'].str.len()\n",
    "    df_translations['en_length'] = df_translations['english'].str.len()\n",
    "    \n",
    "    df_translations['ref_ratio'] = df_translations['ref_length'] / df_translations['en_length']\n",
    "    df_translations['gen_ratio'] = df_translations['gen_length'] / df_translations['en_length']\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    x = df_translations['id']\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, df_translations['ref_ratio'], width, label='Reference', color='steelblue')\n",
    "    ax.bar(x + width/2, df_translations['gen_ratio'], width, label='Generated', color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Sample ID')\n",
    "    ax.set_ylabel('Length Ratio (Hebrew/English)')\n",
    "    ax.set_title('Translation Length Ratio Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='1:1 ratio')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average reference ratio: {df_translations['ref_ratio'].mean():.2f}\")\n",
    "    print(f\"Average generated ratio: {df_translations['gen_ratio'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results\n",
    "\n",
    "Save results for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export translations to CSV\n",
    "if 'df_translations' in locals():\n",
    "    output_file = \"translation_results.csv\"\n",
    "    df_translations.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"✓ Results saved to: {output_file}\")\n",
    "    print(f\"  Columns: {list(df_translations.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Loading and sampling NLLB-200 dataset (5 English-Hebrew pairs)\n",
    "2. ✅ Exploring dataset statistics and distributions\n",
    "3. ✅ Loading model (base or fine-tuned) and generating translations\n",
    "4. ✅ Comparing generated translations with references\n",
    "5. ✅ Visualizing results (lengths, ratios, quality indicators)\n",
    "6. ✅ Interactive testing with custom inputs\n",
    "\n",
    "**Next steps**:\n",
    "- Fine-tune the model: `python train.py --config configs/train_config.yaml`\n",
    "- Update `LORA_ADAPTER` path above to test your fine-tuned model\n",
    "- Use EVALUATION pipeline for comprehensive metrics (BLEU, chrF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
